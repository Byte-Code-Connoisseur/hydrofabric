---
title: "Background"
description: |
  "DevCon 2024 Hydrofabric Workshop"
output:
  distill::distill_article:
    toc: true
    toc_depth: 2
---

```{r setup, include=FALSE}
library(knitr)
knitr::opts_chunk$set(echo = TRUE, 
                      message = FALSE, 
                      warning = FALSE,
                      tidy=TRUE, tidy.opts=list(width.cutoff=60))
```

The hydrofabric team is focused on delivering a consistent, interoperable, flexible, cloud native solution for [hydrofabric data](https://noaa-owp.github.io/hydrofabric/articles/01-intro-deep-dive.html) to those interested in hydrologic modeling and geospatial analysis. _We aim to provide open data to improve open science._ And it doing so strive to make _real data FAIR data_.

In the contexts of DevCon, the hydrofabric provides the foundational features, topology and attributes needed for cartography, web mapping, geospatial analysis, machine learning, model evaluation, data assimilation, and NextGen (and AWI data stream, NGIAB, etc) applications. Outside of DevCon, it provides the needed infrastructure to support other mutli-scale modeling needs (e.g. NHM, US Water Census, Water Balance), vulnerability assessments, and more!

Last year, we shared the concept of a [hydrofabric](https://noaa-owp.github.io/hydrofabric/articles/01-intro-deep-dive.html) and the current of NextGen data structures. A hydrofabric describes the landscape and flow network discretizations, the essential connectivity of network features, and the key reporting locations known as nexus points. Combined these feature serve as both geospatail and computational elements that allow the NextGen modeling infastructure to syncronious different models, formulations, and domains into a cohert simulation and set of outputs.

## Key Highlights

1. **Design Philosophy**: We adopt the OGC [HY Feature conceptual model](https://docs.opengeospatial.org/is/14-111r6/14-111r6.html) with custom modifications for NextGen applications that define an explict data model.
2. **NHGF**: A core, federally consistent data product grounded in a common topology, reference fabric, and set of community POIs. Collectivly, these define a shared NOAA/USGS National Hydrologic Geospatial Fabric.
2. **Network Manipulation**: In-depth exploration of two [network manipulation](https://noaa-owp.github.io/hydrofabric/articles/03-processing-deep-dive.html) processes, refactoring and aggregating, that are crucial for optimizing data usage.
3. **Data Model**: A fundamental data model tailored for modeling and web infrastructure applications, emphasizing efficiency and accuracy through use of modern geospatial (gpkg) and data science (arrow/parquet) formats.This included seven spatial and two a-spatial layers, and future plans for an additional layer for water bodies and cross sections
4. **Data Subsetting**: Demonstrations on how to [extract data subsets](https://noaa-owp.github.io/hydrofabric/articles/05-subsetting.html) to meet multi-scale modeling tasks. for multi-scale modeling tasks using R and a Go-based CLI.
5. **Enriching a hydrofabric**: Optional guidance on accessing and utilizing landscape and river network characteristics to enhance the modeling process.

# Background

The NOAA enterprise hydrofabric is made up of 5 modular components, all of which will be touched on today. These include those seen below:

```{r roadmap, fig.align='center', echo = FALSE, fig.cap="Enterprise Hydrofabric System"}
knitr::include_graphics("../man/figures/roadmap2.png")
```

## Software

`NOAA-OWP/hydrofabric` provides a collection of R packages designed for hydroscience data development and access. These packages share an underlying design philosophy, grammar, and data structures, making them easier to learn and apply together. The packages cover a wide range of data manipulation tasks, from importing and cleaning data, to building custom hydrofabrics, to accessing and summarizing data from 100's of thousands of  to visualizing and modeling it.


```{r}
library(hydrofabric)
```
`library(hydrofabric)` will load the core packages (alphabetical): 

* [climateR](https://github.com/mikejohnson51/climateR) for accessing federated data stores for parameter and attributes estimation
* [hfsubsetR](https://github.com/lynker-spatial/) for cloud-based hydrofabric subsetting
* [hydrofab](https://github.com/mikejohnson51/hydrofab) a tool set for "fabricating" multiscale hydrofabrics
* [ngen.hydrofab](https://github.com/mikejohnson51/ngen.hydrofab) NextGen extensions for hydrofab
* [nhdplusTools](https://github.com/doi-usgs/nhdplusTools/) for network manipulation
* [zonal](https://github.com/mikejohnson51/zonal) for catchment parameter estimation

Additionally it will load key geospatial data science libraries: 

* `dplyr` (data.frames)
* `sf` (vector)
* `terra` (raster)

### Benefits of Using `hydrofabric`

* **Consistency**: Packages are designed to work seamlessly together - with the Lynker-Spatial data stores - making workflows more efficient.

* **Readability**: Syntax is designed to be human-readable and expressive, which helps in writing clean and understandable code.
* **Efficiency**:  Functions are optimized for performance, making data manipulation tasks faster.

## Lynker-Spatial Data

Hydrofabric artifacts are generated from the above reference datasets built in collaboration between NOAA, the USGS, and Lynker for federal water modeling efforts. These artifacts are designed to be easily updated, manipulated, and quality controlled to meet the needs of a wide range of modeling tasks while leveraging the best possible input data.

Cloud-native (modified both in structure and format) artifacts are publicly available through [lynker-spatial](https://www.lynker-spatial.com/) under an [ODbL](https://opendatacommons.org/licenses/odbl/summary/) license. If you use data, please ensure you (1) Attribute Lynker-Spatial, (2) keep the data open, and that (3) any works produced from this data offer that adapted database under the ODbL. 

It follows the general pattern for access: 

```{r pathing, eval = FALSE}
"{source}/{version}/{type}/{domain}_{layer}"
```

Where:

* `source` is the local or s3 location
* `version` is the release number (e.g. v2.2)
* `type` is the type of fabric (e.g. reference, nextgen, etc)
* `domain` is the region of interest (e.g. conus, hawaii, alaska)
* `layer` is the layer of the hydrofabric (e.g. divides, flowlines, network, attributes, etc.)


## Key data structures / storage 

### Storage 

We use [s3 (via aws)](https://docs.aws.amazon.com/AmazonS3/latest/userguide/Welcome.html) for storage that is easy to sync locally, and access remotely: 

```{r sources}
version <-  'v2.2'
type <- "reference"
domain <- "conus"

local_source <- "/Users/mjohnson/hydrofabric"
s3_source    <- "s3://lynker-spatial/hydrofabric"

# Sync s3 with your local archive
(sys <- glue::glue("aws s3 sync {s3_source}/{version}/{type} {local_source}/{version}/{type}"))
```

### GPKG

[Geopackages]((https://www.geopackage.org)/[SQLITE](https://sqlite.org/index.html) is an open, standards-based, platform-independent, and data format for spatial . It is designed to be a universal format for geospatial data storage, enabling the sharing and exchange of spatial data across different systems and software.

```{r gpkg}
gpkg <- "tutorial/poudre.gpkg"

# See Layers
sf::st_layers(gpkg)

# Read Complete Layer
(divides = sf::read_sf(gpkg, "divides"))
```
  
### Arrow/Parquet

Apache [Arrow](https://arrow.apache.org) is an open-source project that provides a columnar **memory** format for flat and hierarchical data. It proviees fast data transfer and processing across different programming languages and platforms without needing to serialize and deserialize the data, making it particularly useful for big data and high-performance applications.

[(geo)parquet](https://parquet.apache.org/docs/overview/) is an **on disc** data format for storing columar data. GeoParquet is an emerging standard for storing geospatial data within the Apache Parquet file format. Parquet is a columnar storage file format that is highly efficient for both storage and retrieval, particularly suited for big data and analytics applications.

We distribute hydrofabric layers as VPU-based hive partitioned (geo)parquet stores. These can be accessed from lynker-spatial, or, synced (see above) to a local directory. Hive partitioning is a partitioning strategy that is used to split a table into multiple files based on partition keys. The files are organized into folders.  The complete `v2.2/reference/` directory is ~3.0GB.

  
#### Parquet store

```{r}
(x <- glue::glue("{local_source}/{version}/{type}/{domain}_network"))
arrow::open_dataset(x)

#> Remote parity
#> open_dataset(glue('{s3_source}/{version}/{type}/{domain}_network/'))
```

####  Geoparquet store

```{r}
(x <- glue::glue("{local_source}/{version}/{type}/{domain}_divides"))
arrow::open_dataset(x)

#> Renote parity
#>  arrow::open_dataset(glue::glue("{s3_source}/{version}/{type}/conus_divides"))
```

## Parquet Schema

The [pqrs](https://github.com/manojkarthick/pqrs) library offers a command line tool of inspecting Parquet files

```{bash, eval = FALSE}
pqrs schema /Users/mjohnson/hydrofabric/v2.2/reference/conus_flowlines/vpuid=01/part-0.parquet
```

```{r, echo = FALSE}
knitr::include_graphics('../man/figures/pqrs_nice_formating.png')
```



### Virtual Access to Gridded Data

The amount of gridded data needed for the suite of our supported applications (let alone)
  - [GDAL VSI](https://www.google.com/url?sa=t&rct=j&q=&esrc=s&source=web&cd=&cad=rja&uact=8&ved=2ahUKEwil-fWKx_D-AhWCIzQIHXIHDD4QwqsBegQIAxAE&url=https%3A%2F%2Fwww.youtube.com%2Fwatch%3Fv%3DauK_gPR-e7M&usg=AOvVaw2ITVtXkwdDj5PCzIfSQwbW) 
  
```{r}
'/vsis3/lynker-spatial/gridded-resources/medium_range.forcing.tif' |> 
  climateR::dap(AOI = divides) |>
  terra::plot()
```
 
## Lazy Evaluation

All datasets are distributed at the `domain` level (e.g. conus, hawaii, alaska). [Lazy evaluation](https://arrow.apache.org/cookbook/r/manipulating-data---tables.html) can help you get just the data you need, in memory from local or remote locations.
  
#### Local GPKG

```{r}
hydrofabric::as_sqlite(gpkg,  "divides") %>% 
  dplyr::filter(divide_id == 2896607)

hydrofabric::as_sqlite(gpkg,  "divides") %>% 
  dplyr::filter(divide_id == 2896607) %>% 
  hydrofabric::read_sf_dataset_sqlite()
```
  
#### Parquet Store

```{r}
arrow::open_dataset(glue::glue('{local_source}/{version}/{type}/conus_network/')) %>% 
  dplyr::filter(id == 101) %>% 
  dplyr::select(id, toid) %>% 
  dplyr::collect()

#> arrow::open_dataset(glue::glue('{s3_source}/{version}/{type}/conus_network/')) %>% 
#>   dplyr::filter(id == 101) %>% 
#>   dplyr::select(id, toid) %>% 
#>   dplyr::collect()
```

## Quick R Notes

```{r}
## NOTE: What is glue?
x <- "Welcome to DevCon"
y <- "2024"
z <- c('2023', '2024', '2025', '...')

# single string
glue("{x} {y}")

# multiple strings from vectors
glue("{x} {z}")
```

```{r, eval = FALSE}
## NOTE: Finding help/docs
?read_hydrofabric
```