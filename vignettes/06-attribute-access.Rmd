---
title: "Attribute Access and Creation"
description: |
  "Adding scientific value to data fabrics"
author:
  - name: "Mike Johnson"
    url: https://github.com/mikejohnson51
    affiliation: Lynker, NOAA-Affiliate
    affiliation_url: https://lynker.com
output: distill::distill_article
---


```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(hydrofabric)
```

We have established how to build a subset hydrofabric product. Now, we are focused on how to add extra information to this data to support modeling and science applications.

We will focus on three high level examples:

1. Precomputed data
2. Building your own
3. Leveraging legacy data


# 1. Precomputed Data 

The intention with NextGen is to provide a suite of precomputed, useful information related to each release of the hydrofabric. To date these include the attributes needed to run CFE/NOAH-OWP, and will eventually include the attributes found in the CAMELS dataset.

Precomputed data with live in the same directory as the released hydrofabric artifacts and will follow the same VPU segmentation.

Here, we can see how to extract information for CFE for our Poudre subset.

### Set up

```{r}
library(arrow)

# read network
x = read_sf("cihro-data/poudre.gpkg", "network")

# define variable set
attr = 'cfe_noahowp'

# define path 
p = glue("s3://lynker-spatial/pre-release/nextgen_{x$vpu[1]}_{attr}.parquet")
```

### Access and Join

```{r}
divides = read_sf("cihro-data/poudre.gpkg", "divides")

# Open dataset and Join to divides
divides = open_dataset(p) %>% 
  filter(divide_id %in% divides$divide_id) %>% 
  collect() %>% 
  right_join(divides, by = "divide_id") %>% 
  st_as_sf()
```

# Explore

```{r}
glimpse(divides)
```

```{r, echo = FALSE}
plot(divides[grep('quartz_soil_layers', names(divides), value = TRUE)], border = NA)

plot(divides[grep('smcmax', names(divides), value = TRUE)], border = NA)
```


# 2. Unique Use Cases

Now, you may not want to use (or be limited) by the data we defined. That is ok too!

As part of the enterprise hydrofabric utilities/aspirations, we provide tools to access over 100,000 different datasets without requiring you to download or find the data!

| Repo        | Purpose           | 
| ------------- |:-------------:| 
| mikejohnson51/climateR      | Tools for for accessing remote data resources for parameter and attributes estimation | 
| mikejohnson51/climateR-catalogs      | Flat data catalog of virtually accessible resources | 
| NOAA-OWP/zonal      | Tools for rapid areal summarization |


## Gridmet solar radiation on 2020-10-29

### Single Day

```{r}
r = getGridMET(AOI = divides, varname = "srad", startDate = "2020-10-29")

plot(r$daily_mean_shortwave_radiation_at_surface)

srad = execute_zonal(r, geom = divides, fun = "mean", ID = "divide_id")

plot(srad['mean.srad_2020.10.29'])
```

### Through time
```{r}
p = getGridMET(AOI = divides, varname = "pr", startDate = "2023-05-10", endDate = "2023-05-13")

plot(p$precipitation_amount)

pr = execute_zonal(p, geom = divides, fun = "mean", ID = "divide_id")

plot(pr[grep('pr', names(pr), value = TRUE)], border = FALSE)
```

### On the fly exploration

Here we want rank map the NextGen Catchments by the amount of rain they received over the last few days

```{r}
pr$cumpr = rank(st_drop_geometry(pr[grep('pr', names(pr), value = TRUE)]) %>% rowSums())

plot(pr['cumpr'])
```

### Beyond Gridmet

climateR provides a range of short cut functions to common datasets. These are simply shortcuts to make access faster:


```{r}
grep("get", ls("package:climateR"), value = TRUE)
```

#### Mean Monthly Normals

```{r}
ppt = getTerraClimNormals(AOI = divides,  varname = "ppt") %>% 
  execute_zonal(geom = divides, ID = "divide_id")

plot(ppt[grep("ppt", names(ppt), value = TRUE)], border = NA)
```

### Beyond shortcut functions

The climateR-catalogs provide a vast catalog of resources that can be accessed.

If you can find the catalog element you want, it can be passed to the more general `dap` utilities.

```{r}
glimpse(read_live_catalog())
```

### POLARIS Soils

```{r}
cat = filter(catalog, id == "polaris", grepl("mean alpha", variable))

alpha = dap(catalog = cat, AOI = divides)
names(alpha) = paste0("layer", 1:nlyr(alpha))

plot(alpha)
```

```{r}
soil = execute_zonal(alpha, geom = divides, fun = "mean", ID = "divide_id")

plot(soil[grepl('mean.layer', names(soil))], border = NA)
```

### USGS DEMs

```{r}
(cat = catalog %>%  filter(asset == "30m CONUS DEM"))

dem = dap(catalog = cat, AOI = divides)

plot(dem[[1]])
```


### Beyond R

The USGS [gdptools](https://code.usgs.gov/wma/nhgf/toolsteam/gdptools) provides similar climateR/zonal capabilities over the same data catalog in Python

# 3. Legacy of USGS based COMID data

There is a wide range of legacy and new products produced by the USGS that rigorously summarize 1,000's of attributes to the NHDPlusV2 catchment network.

One of these that is of particular interest to NextGen is [TWI](https://www.sciencebase.gov/catalog/item/56f97be4e4b0a6037df06b70). 

While TOPMODEL requires a distribution, a quick and dirty way to get NextGen catchment TWI estimates is of immense value.


```{r}
knitr::include_graphics("../man/figures/usgs-twi.png")
```


## TWI joined to the Poudre River Network

```{r}
net = read_sf("cihro-data/poudre.gpkg", "network") 

twi = data.frame(comid = unique(net$hf_id)) %>% 
  left_join(get_vaa('areasqkm'), by = "comid") %>% 
  left_join(read_parquet('cihro-data/56f97be4e4b0a6037df06b70_tot.parquet'), by = c("comid"="COMID")) %>% 
  select(hf_id = comid, s_areasqkm = areasqkm, TOT_TWI)

```


## Reference fabric data

```{r}
ref = get_fabric(VPU = "10L", base_s3 = 's3://lynker-spatial/01_reference/', cache_dir  = "cihro-data") %>% 
  read_sf("reference_catchment") %>% 
  filter(FEATUREID %in% twi$hf_id) %>% 
  left_join(twi, by = c("FEATUREID"="hf_id"))

plot(ref['TOT_TWI'], border = NA, main = paste(nrow(ref), " reference catchments"))
```

## Area averaging reference fabric to NextGen

```{r}
full = net %>% 
  select(divide_id, hf_id, areasqkm) %>% 
  distinct() %>% 
  left_join(twi, by = "hf_id")


f1 = aggregate_zones(data = full, 
                     geom = NULL,  
                     crosswalk = select(full, hf_id, divide_id, areasqkm, s_areasqkm),
                     ID = "divide_id") %>% 
  right_join(divides, 
             by = "divide_id") %>% 
  st_as_sf()


plot(f1['TOT_TWI'], border = NA, main = paste(nrow(f1), "nextgen catchments"))
```

We are not limited to just TWI. The nhdplusTools package provides a archive of NHDPlusV2 based catchment datasets that can be used:

```{r}
xx = discover_nldi_characteristics("all") 

lapply(xx, nrow)

xx$total$characteristic_id[1:10]
```

Further Lynker provides a reformatted archive of the EPA streamcat data:

```{r}
xx = aws.s3::get_bucket_df(bucket = 's3://nextgen-hydrofabric', prefix = 'streamcats') %>% 
  filter(!grepl("old|data", Key))

nrow(xx)

xx$Key[2:11]
```


## Summary

- NOAA-OWP/hydrofabric provides network subsetting function and preloads a number of software tools critical to the enterprise effort

 - The reference fabric built on the NHDPlusV2 allows for a minimal, improved network that can be manipulate through refactoring and aggregation to meet the needs of a variety of modeling tasks

 - NHDPlusV2 is the current, but just only possible reference, however, because of the NHDPlusV2 legacy the data for a Web infrastructure is available.
 
 - The NextGen data fabrics are one form of a refactored/aggregated network with extensions for the the modeling application.

 - For those who "just" want to use the data we have created, a Go based API (that runs hydrofab::subset_network) is available
  
 - For those who who want to make there own, all tools are available and open
 
 - We also provide the tools to start enhancing the data fabrics with information that helps guide scientific choices, model options, and understanding
    
