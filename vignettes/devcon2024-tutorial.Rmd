---
title: "End to End Hydrofabric Workflow"
description: |
  "DevCon 2024"
author:
  - name: "Mike Johnson"
    url: https://mikejohnson51.github.io/
    affiliation: Lynker, NOAA-Affiliate
    affiliation_url: https://lynker.com
  - name: "Justin Singh-Mohudpur"
    url: https://github.com/program--
    affiliation: Lynker, NOAA-Affiliate
    affiliation_url: https://lynker.com
  - name: "Arash Modaresi Rad"
    url: https://github.com/arashmodrad 
    affiliation: Lynker, NOAA-Affiliate
    affiliation_url: https://lynker.com
  - name: "James Coll"
    url: https://github.com/james.coll
    affiliation: Lynker, NOAA-Affiliate
    affiliation_url: https://lynker.com
output: distill::distill_article
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, out.width = "100%", message = FALSE, warning = FALSE)

hls = arrow::open_dataset(glue::glue("/Users/mjohnson/hydrofabric/v2.2/conus_hl")) |>
  dplyr::select(count) |>
  dplyr::collect() |>
  nrow()
```

# High level

The HydroFabric team is dedicated to delivering a consistent, interoperable, flexible, cloud-native solution for hydrologic modeling.

This solution provides foundational features, topology, and attributes necessary for a variety of applications, including mapping, geospatial analysis, machine learning, evaluation, data assimilation, NextGen, AWI data stream, NGIAB, and other modeling needs such as the National Hydrologic Model (NHM) and the U.S. Water Census

```{r, fig.align='center', echo = FALSE, fig.cap="Enterprise Hydrofabric System"}
knitr::include_graphics("../man/figures/roadmap2.png")
```

# Software

Integrated software stack

# Background 

Key data structures here

Heavy use of 

  - [GPKG](https://www.geopackage.org) / [SQLITE](https://sqlite.org/index.html) (hive partitioning)
  - [Arrow](https://arrow.apache.org)/[(geo)Parquet](https://parquet.apache.org/docs/overview/)
  - [GDAL VSI](https://www.google.com/url?sa=t&rct=j&q=&esrc=s&source=web&cd=&cad=rja&uact=8&ved=2ahUKEwil-fWKx_D-AhWCIzQIHXIHDD4QwqsBegQIAxAE&url=https%3A%2F%2Fwww.youtube.com%2Fwatch%3Fv%3DauK_gPR-e7M&usg=AOvVaw2ITVtXkwdDj5PCzIfSQwbW) 
  - s3 (aws)
  - defered evaluation 

Today's tutorial makes a concentrated effort to highlight these features using a real-world example.

While we provide a national 
For this example, we will use NWIS `06752260` that sits on the Cache La Poudre River in Fort Collins, Colorado. You can use any USGS gage you desire, or any of the `r hls` found via [lynker-spatial](https://lynker-spatial.s3-us-west-2.amazonaws.com/hydrofabric/v2.2/hydrolocations.html).

  
```{r, echo = FALSE}
knitr::include_graphics("../man/figures/hydrofabric.png")
```

Successfully completing this tutorial will equip you with the data products (and skills!) needed to run the AWI datastream and NGIAB.

# Setup 
  
```{r, eval = FALSE}
# Install -----------------------------------------------------------------
# install.packages("remotes") 
remotes::install_github("NOAA-OWP/hydrofabric")
# powerjoin
# bit64
```

# Attach Package

```{r}
library(hydrofabric)
library(bit64)
library(powerjoin)
```

# Helpers

```{r}
make_map = function(file, pois) {
  hf = read_hydrofabric(file)
  mapview::mapview(hf$catchments) + hf$flowpaths + pois
}

## NOTE: What is glue?
x <- "Welcome to DevCon"
y <- "2024"

glue("{x} {y}")
```

```{r, eval = FALSE}
## NOTE: Finding help/docs
?read_hydrofabric
```

# Data Stores 

```{r}
hf_version <-  2.2
(local_example <- glue("/Users/mjohnson/hydrofabric/v{hf_version}"))
(s3_example    <- glue("s3://lynker-spatial/hydrofabric/v{hf_version}"))
```

```{r}
fs::dir_tree(local_example, recurse = FALSE)

fs::dir_tree(glue("{local_example}/reference"), recurse = FALSE)
```

# Geoparquet store

```{r}
# Geoparquet store local
fs::dir_tree(glue("{local_example}/reference/conus_divides"), recurse = FALSE)
open_dataset(glue("{local_example}/reference/conus_divides"))
```

# Parquet store

```{r}
open_dataset(glue("{local_example}/reference/conus_network"))
```

```{r}
open_dataset(glue('{s3_example}/reference/conus_divides/'))
```

# Deferred Evaluation

```{r}
open_dataset(glue('{s3_example}/reference/conus_network/')) %>% 
  filter(id == 101) %>% 
  select(id, toid)
```

```{r}
open_dataset(glue('{s3_example}/reference/conus_network/')) %>% 
  filter(id == 101) %>% 
  select(id, toid) %>% 
  collect()
```

```{r}
open_dataset(glue('{local_example}/reference/conus_network/')) %>% 
  filter(id == 101) %>% 
  select(id, toid) %>% 
  collect()
```

```{r}
(sys <- glue("aws s3 sync {s3_example} {local_example}"))
```

```{r, eval = FALSE}
system(sys)
```


```{r}
### ---- Sample outfiles for today ---- ###
fs::dir_create("vignettes/tutorial")

using_local_example    <- '/Users/mjohnson/hydrofabric'

reference_file  <- "vignettes/tutorial/poudre.gpkg"
refactored_file <- "vignettes/tutorial/refactored.gpkg"
aggregated_file <- "vignettes/tutorial/aggregated.gpkg"


nextgen_file       <- "vignettes/tutorial/poudre_ng.gpkg"
model_atts_file    <- "vignettes/tutorial/poudre_ng_attributes.parquet"
model_weights_file <- "vignettes/tutorial/poudre_ng_weights.parquet"
```

# Get Reference Fabric

```{r}
## ---  Define starting feature by source and ID
## https://waterdata.usgs.gov/monitoring-location/06752260
## https://reference.geoconnex.us/collections/gages/items?provider_id=06752260

(gage <- list(featureSource = "nwis", featureID = "06752260"))

# Use get_subset to build a reference subset
get_subset(
  nldi_feature = gage,
  source  = using_local_example,
  type = "reference",
  hf_version = "2.2",
  outfile = reference_file,
  overwrite = TRUE
)
```

# Get some Points of Interest

```{r}
hf = read_hydrofabric(reference_file)

pois = open_dataset(glue("{using_local_example}/v2.2/conus_hl")) %>%
  filter(hl_source == 'GFv20') %>%
  collect() %>%
  st_as_sf(coords = c("X", "Y"), crs = 5070) %>%
  st_filter(hf$catchments)

make_map(reference_file, pois)
```


# Build a Refactored Fabric

```{r}
refactored = refactor(
  reference_file,
  split_flines_meters = 10000,
  collapse_flines_meters = 1000,
  collapse_flines_main_meters = 1000,
  pois = pois,
  fac = '/vsis3/lynker-spatial/gridded-resources/fac.vrt',
  fdr = '/vsis3/lynker-spatial/gridded-resources/fdr.vrt',
  outfile = refactored_file
)

make_map(refactored_file, pois)
```


# Build an Aggregated Network 
```{r}
hydrolocations = read_sf(refactored_file, 'lookup_table') %>%
  inner_join(pois, by = c("NHDPlusV2_COMID" = "hf_id")) %>%
  select(poi_id, NHDPlusV2_COMID, id = reconciled_ID) %>%
  distinct()

head(hydrolocations)
```

```{r}
aggregate_to_distribution(
  gpkg = refactored_file,
  hydrolocations = hydrolocations,
  ideal_size_sqkm = 10,
  min_length_km = 1,
  min_area_sqkm = 3,
  outfile = aggregated_file,
  overwrite = TRUE )

make_map(aggregated_file, pois)
```

# Generate a NextGen Network

```{r}
unlink(nextgen_file)

apply_nexus_topology(aggregated_file, export_gpkg = nextgen_file)

hf = read_hydrofabric(nextgen_file)

mapview::mapview(hf$catchments) + 
hf$flowpaths + 
read_sf(nextgen_file, "nexus")
```

# Populate Data Needed for CFE/NOM/PET

```{r}
vsi      <- "/vsis3/lynker-spatial/gridded-resources"
div <- read_sf(nextgen_file, "divides")
```

## X Y (for forcing downscaling)
```{r}
 d1 <- st_centroid(div) |>
    st_transform(4326) |>
    st_coordinates() |>
    data.frame() |>
    mutate(divide_id = div$divide_id)

```

## Elevation data for Forcing downscaling and NOAH-OWP

```{r}
dem_vars <- c("elev", "slope", "aspect")

r  <- rast(glue('{vsi}/250m_grids/usgs_250m_{dem_vars}.tif'))

d2 <- execute_zonal(r[[1:2]], 
                    div, ID = "divide_id", 
                    join = FALSE) |>
    setNames(c("divide_id", "elevation_mean", " slope"))

  
d3 <- execute_zonal(r[[3]], 
                     div, ID = "divide_id", fun = circular_mean, 
                     join = FALSE) |>
    setNames(c("divide_id", "aspect_c_mean"))
```

## NOAH OWP Varibables 

```{r}
nom_vars <- c("bexp", "dksat", "psisat", "smcmax", "smcwlt")

d4 <- execute_zonal(rast(glue("{vsi}/nwm/conus/{nom_vars}.tif"), 
                    lyrs = seq(1,length(nom_vars)*4, by = 4)), 
                    div, ID = "divide_id", join = FALSE)  %>% 
    setNames(gsub("mean.", "", names(.)))
```
 
# GW Routing parameters

```{r}
crosswalk <- as_sqlite(nextgen_file, "network") |>
    select(hf_id, divide_id) |>
    collect()

d5 <- open_dataset("s3://lynker-spatial/hydrofabric/v2.2/reference/routelink_ls/") |>
    select(hf_id , starts_with("ml_")) |>
    inner_join(mutate(crosswalk, hf_id = as.integer64(hf_id)), by = "hf_id") |>
    group_by(divide_id) |>
    collect() |>
    summarize(
      gw_Coeff = round(weighted.mean(gw_Coeff, w = gw_Area_sqkm, na.rm = TRUE), 9),
      gw_Zmax  = round(weighted.mean(gw_Zmax,  w = gw_Area_sqkm, na.rm = TRUE), 9),
      gw_Expon = mode(floor(gw_Expon))
    )
```
 
```{r}
model_attributes <- power_full_join(list(d1, d2, d3, d4, d5), by = "divide_id")
  
write_parquet(model_attributes, model_atts_file)
```
  

# Weight Grids 

```{r}
type = "medium_range.forcing"

w = weight_grid(rast(glue('{vsi}/{type}.tif')), div, ID = "divide_id") |> 
  mutate(grid_id = type)

head(w)

write_parquet(w, model_weights_file)
```

oh BTW area was been calculated in Rl_ls

# Extacting Cross Sections 

We introduce the 3D Cross Sections Project, which provides transects along the entire river network in the CONUS (Contiguous United States). This product is developed using and indexed to the National Hydrologic Geospatial Fabric Reference [link](https://www.sciencebase.gov/catalog/item/60be0e53d34e86b93891012b) and is currently available in an optimized cloud-native format at [link](s3://lynker-spatial/hydrofabric/v20.1/3D/cross-sections/). The product is built on a 10m DEM from the 3D Elevation Program ([3DEP](https://www.usgs.gov/3d-elevation-program)) and is supplemented by machine learning models to account for missing bathymetry, covering all 2.7 million reaches in the CONUS.

In this context, we demonstrate how to use a subset of the NextGen file to locate the outlet, retrieve the ID, and pull machine-learned bathymetry information while automatically handling the crosswalk between different geospatial fabrics. We also illustrate the use of the [AHGestimation package](https://github.com/mikejohnson51/AHGestimation) to generate the missing bathymetric information. This package offers numerous functionalities, including the computation of mass-preserving hydraulic geometries, rating curves, generating cross sections from at-a-station hydraulic geometry (AHG) principles, and more.


```{r}
crosswalk <- as_sqlite(nextgen_file, "network") |>
    select(hf_id, id, divide_id, hydroseq, poi_id) |>
    filter(!is.na(poi_id)) %>% 
    collect() %>% 
    slice_min(hydroseq)

cs <- open_dataset("s3://lynker-spatial/hydrofabric/v2.2/reference/routelink_ls/") |>
    select(hf_id, TopWdthCC, owp_dingman_r, owp_y_bf) |>
    inner_join(mutate(crosswalk, hf_id = as.integer64(hf_id)), by = "hf_id") |>
    collect() %>% 
    summarise(TW = mean(ml_tw_nchan(m)),
              r = mean(ml_r),
              Y = mean(ml_y_inchan(m)),
              poi_id = poi_id[1])

bathy = AHGestimation::cross_section(r = cs$r, TW = cs$TW, Ymax = cs$Y) 

plot(bathy$x, bathy$Y, type = "l", 
     ylab = "Releative distance (m)", 
     xlab = "Depth (m)", 
     main = glue("Average XS at POI: {cs$poi_id}"))
```

For more details regarding the AHGestimation toolbox, please refer to our [JOSS publication](https://joss.theoj.org/papers/10.21105/joss.06145). For information on the machine learning methods, see our under review [publication](https://lynker-spatial.s3.us-west-2.amazonaws.com/documents/ml_manuscript.pdf). For further demonstrations, visit our [page](https://noaa-owp.github.io/hydrofabric/).

# Populate Flowpath Attributes 

```{r}
add_flowpath_attributes(nextgen_file)
```

```{r}
# Schema, showing as_sqlite utility
as_sqlite(nextgen_file)

# Data
as_sqlite(nextgen_file, 'flowpath_attributes')
```


# Make it pretty 

Adding symbology

```{r}
append_style(nextgen_file, layer_names = c("divides", "flowpaths", "nexus"))
```




